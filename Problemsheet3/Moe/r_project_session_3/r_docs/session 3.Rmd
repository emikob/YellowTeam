---
title: "session 3"
author: "Moritz Braun"
date: "26 10 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(tidyverse)
```

# 1. Revision

## 1.1. Getting data into R

Read-in `TheDataSet.csv` as a data frame called `TheDataSet`. Don’t forget that you have done this before successfully!

```{r}
TheDataSet <- read.table("../data/TheDataSet.csv", sep ="", header = TRUE)

```

Have a look at the first rows of your data to make sure everything was loaded properly.

```{r}
head(TheDataSet,n = 10) %>%
  knitr::kable(digits = 2)
```

## 1.2. Exploring data

Remind yourself what kind of variables you had in the data by summarizing it.

```{r}
summary(TheDataSet)
```

# 2. Sampling, creating data and aggregating

One of the best ways to understand many inferential procedures is to actually create your own data and see how the outcome of the statistical test changes in dependence of changes in the data. Here we will go through some of the most important tools necessary for getting this implemented in R.

## 2.1. Probability sampling

In order to draw a subset of the available data you can use the function `sample`.

```{r}
#?sample
```

Using the example from the help function, create a vector with twelve values and sample 6 values. Try it without and with replacement.

```{r}
#create a vector with 12 elements
x <- 1:12
#random sample of 6 elements from the vector x without replacement
sample(x, 'size' = 6)
```

```{r}
#random sample of 6 elements from the vector x with replacement
sample(x, 'size' = 6, 'replace' = TRUE)
```

Evaluate the `sample` function repeatedly. You will notice that it will return different values. In order to generate more reproducible samples (e.g. every time you run the function it samples the same 6 random numbers) you can set the seed of the random number generator. This generator is not *truly* random but *pseudo-random* as it is generated by an algorithm that returns the same pseudo-random sequence for each start seed. We can use the `set.seed` function to specify the seed value. The value can be any numeric value. Check out the *Note* in the help function for `set.seed` to find out more details. Repeatedly set the seed to 44 and sample 6 values - `sample` should return the same 6 values every time.

```{r}
set.seed(44)
sample(x, 'size' = 6, 'replace' = TRUE)
```
Finally, we can also sample with a predefined probability. In this case, we might want to sample from a student population in which 25% of the students failed their exam. Note, that setting these probability weights will start converging to 25% when using large samples.

```{r}
sample(c("pass", "fail"), 10, replace = TRUE, prob = c(.75, .25))
```

## 2.2. Generate non-random data

In simple cases, you can just repeat any vector.

```{r}
rep(c("pass", "fail"), times = 5)
```

Play around with the arguments `each` and `times` in this function to get an intuition about the different possibilities it offers. Also have a look at the *Examples* section in the help page of `rep`.

```{r}
#?rep

#times defines the number of times that the vector is repeated
rep(c("pass", "fail"), times = 10)

#each defines how often an element is repeated before the next element is taken
rep(c("pass", "fail"), each = 5)


rep(c("pass", "fail"), times = 5, each = 2)
```

In cases where we want to generate regular sequences, we can use seq.

```{r}
seq(from = 1, to=10, by = 1)
```
```{r}
seq(from = 1, to=10, by = 0.5)
```

## 2.3. Random data generation

For now we will focus on generating values which follow a uniform or normal distribution. For this we will use:

```{r}
# uniform distribution
runif(6)
```

```{r}
# normal distribution
rnorm(6)
```

Now generate 2 different vectors of 100 random numbers. One from a normal distribution with a mean of 0 and a standard deviation of 2, and one from a uniform distribution with a minimum of -3 and a maximums of 3. Quickly plot these two using hist or a combination of `plot` and `density`.

```{r}
# uniform distribution
uniform <- runif(100, min = -3, max = 3)
# normal distribution
normal <- rnorm(100, mean = 0, sd = 2)
```

```{r}
#histogram
hist(uniform)
#density plot
plot(density(uniform))
```

```{r}
#histogram
hist(normal)
#density plot
plot(density(normal))
```

## 2.3. Creating a data set

Now we will put a few of the functions from above together, in order to create our very first fake data set. For this data set we collected a hundred Pokémon (feel free to refer to them as Pokéfriends) of 3 different types and measured their power and friendliness.


```{r}
# should there be time instead of each?
set.seed(88)
sandbox <- data.frame(
  pokemon=factor(rep(c("Charizard",
                       "Pikachu",
                       "Eevee"), each=100)),
  power=c(rnorm(100, mean=60, sd=2),
          rnorm(100, mean=55, sd=2),
          rnorm(100, mean=50, sd=2)),
  friendliness=c(rnorm(100, mean=10, sd=2),
               rnorm(100, mean=15, sd=2),
               rnorm(100, mean=20, sd=2))
  )
```

Again, you can look at some summary values and also plot the data distributions in order to evaluate if it meets the parameters that you have set when you created it. Besides online resources, you can also use the **Histogram-Fun** html file created by a former classmates in the **week2** folder for help.

```{r}
summary(sandbox)
ggplot(sandbox, aes(x=power, color=power, fill=pokemon)) +
geom_density() + geom_density(aes(x=friendliness, color=friendliness, fill=pokemon),alpha=0.5)
# the more transparent density plots are the friendliness values and the full one are the power values
```

## 2.4. Aggregating

As with any other language, there are many ways to achieve a certain goal. For data aggregation I will be using the function `aggregate`. I like it because it maintains the same syntax as many other functions we will be using in the future. Note, however, that people also use `by`, `ddply`, `group_by`, and many others for the same purpose. Once you understand the basic principals of aggregation you can pick whatever function feels most natural to you, or whichever is the most fitting for your current needs.

```{r}
#with aggregate
(aggPokemon <- aggregate(data = sandbox, FUN = mean, friendliness ~ pokemon))

#with group_by
aggPokemon2 <- sandbox %>%
  group_by(pokemon) %>%
  summarise(friendliness=mean(friendliness))
aggPokemon2

#DIFFERENT FORMATS IN RESULTS???
```

If this makes it a bit more intuitive, you can read the arguments of the function as “From the data frame `sandbox`, aggregate the **mean** (this can be a different function, e.g. `median`, `sd`, `sum`) of the variable **friendliness** as a function of (**~**) the variable **pokemon**.

Now try to aggregate the mean of the **power** variable as a function of Pokémon type. You can also use `cbind` to aggregate both **friendliness** and **power** in the same data frame. Look at the Examples in the help function of `aggregate` for inspiration.

```{r}
(aggPokemon <- aggregate(power ~ pokemon, FUN = mean, data = sandbox))
```
```{r}
#?aggregate
```

```{r}
(aggPokemon <- aggregate(cbind(friendliness, power) ~ pokemon, FUN = mean, data = sandbox))
```

Hopefully the aggregated values match the values that you used to create that data set. It won’t be an exact match - as you only sampled 100 data points out of the specific distribution, but with an increasing sample size the values will converge.

# 3. Correlation

## 3.1. Scatterplot

Before running a correlation on our data it might make sense to visualize it. A good way to look at the relationships between variables is the scatterplot. This plot will give us a first impression about the kind of relationship there is between variables. It further tell us if there are any outliers.

From `ggplot2` you can use `geom_point` for this purpose.

Please try and plot **friendliness** on the y-axis and **power** on the x-axis.

```{r}
ggplot(sandbox, aes(power, friendliness)) + 
  geom_point(aes(colour = pokemon))
```

Do you think there is an interesting relationship in this data? Lets see if the variables are correlated.

## 3.2. From covariance to correlation

In this session we will have a closer look at multiple ways of measuring relationships between variables and how to interpret them.

### 3.2.1. Covariance

A simple method of quantifying the association between variables is to investigate how they covary. Covariance uses the relationship between the variance of two variables - as one variable deviates from its mean (variance), does the other variable deviate from its mean? If it does (either in the same, or opposite way) then the two variables are related (have a look at the [formula](http://www.r-tutor.com/elementary-statistics/numerical-measures/covariance) if you want).

A *positive* covariance indicates that as one variable deviates from the mean, the other ones deviates in the same direction, e.g. as **friendliness** increases, **power** also increases. If the other variable deviates from the mean in the opposite direction, this would indicate a negative covariance.

Use the `cov` function to compute the covariance between **friendliness** and **power**. Is it positive or negative? Is the output expected after seeing the plot?

```{r}
cov(sandbox$friendliness, sandbox$power)
```

### 3.2.2. Correlation

The issue with covariance is that it is not a standardized measure - it will completely depend on the scale of the measurement you used. This makes it difficult to compare it across different metrics. In other words, telling me that there is a negative covariance of -15 does not tell me much about the strength of the relationship when I compare it to a covariance in another data set. This would only be informative if everything, always was measured in the same units. The standardized covariance is known as a correlation coefficient - it is the covariance divided by the product of the individual standard deviations of your two variables ( [formula](http://www.r-tutor.com/elementary-statistics/numerical-measures/correlation-coefficient)).

Correlations will give us values between -1 (perfectly negatively correlated) and 1 (perfectly positively correlated). A zero coefficient indicates, that there is no linear relationship between the variables - as one variable changes, the other one stays the same.

Use the `cor` function to compute the correlation between **friendliness** and **power**. The syntax is the same as for `cov`.

```{r}
cor(sandbox$friendliness, sandbox$power)
```

The resulting correlation coefficient is quite large and negative, indicating that as one variable increases the other decreases quite substantially. As this coefficient is a standardized measure of the observed effect, one can use it as a measure of effect size.

In order to test if this value is significantly different from 0 (i.e. no relationship between the variables), we can either calculate it ourselves, or use the `cor.test` function. In order to calculate it yourself you can just translate the appropriate [formula](https://stats.stackexchange.com/questions/270612/why-test-statistic-for-the-pearson-correlation-coefficient-is-frac-r-sqrtn-2) into R code. First, we want to save the correlation coefficient in a separate object.

```{r}
corrcoef <- cor(sandbox$friendliness, sandbox$power)
```

This makes it easier to handle. Now we can translate the formula into code.

```{r}
tvalue <- ((corrcoef)*sqrt(nrow(sandbox)-2))/sqrt(1-(corrcoef)^2)
print(paste("Our t-value is:", tvalue, sep = " "))
```

You can look up the t-value in the critical values of the [t distribution](http://home.ubalt.edu/ntsbarsh/Business-stat/StatistialTables.pdf) to obtain a p-value.

But now, please use the `cor.test` function to compare the output. The syntax is the same as for `cor`.

```{r}
cor.test(sandbox$friendliness, sandbox$power)
```

### 3.2.2. Interpreting the correlation output

By default, `cor.test` produces Pearson’s r and a 95% confidence interval. We can see the t-value, degrees of freedom and the p-value. Critically, the 95 confidence range does not include zero - the population/actual values of the correlation are likely negative. The correlation coefficient is given at the bottom of the output. We can further quantify how much variance is shared between the variables. R^2 tells us how much of the variability in **power** is shared by **friendliness**.

```{r}
Rsquare <- cor(sandbox$friendliness, sandbox$power)^2
Rsquare
```

So although power was highly correlated with friendliness, it can account for *only* 61% of the variation in friendliness. Almost 40% of the variability needs to be accounted for by other variables. As you can remember from your undergrad - this does not imply causation.

### 3.2.3. Non-parametric correlation (Spearman's r)

In cases where normality has been violated, we can use non-parametric measures, such as the Spearman’s correlation coefficient. These kinds of procedures work, by first ranking the data and then applying Pearson’s method from above. Fortunately, all you have to do is replicate what you did with `cor.test`, but change the default method from “pearson” to *“spearman”*. You can always use the help function for, well, help.

```{r}
cor.test(sandbox$friendliness, sandbox$power, method = 'spearman')
```

This method does not return a confidence interval, as this needs to be bootstrapped (but more about this in later sessions). Note, that we now speak of a **rho** (pronounced “row”) value instead of *r*.


### 3.2.4. Issues

Due to the fact that we actually generated our data set, you might have already noticed some issues we might have when writing our new paper for the highest ranking Pokémon journal. Depending on the exact research question, it is likely we have ignored a pretty important part of our data set - our variables are sampled from different distributions for each Pokémon type. If we ignore this, it seems we have good evidence for a negative relationship between power and friendliness. But does ignoring the actual type of Pokémon make sense? More importantly, does the negative relationship hold true *within* each of the Pokémon types?

There are many ways to address this question, and we will do so in detail next session, but for now I want you to quantify the relationship between power and friendliness for each of the Pokémon types individually!

To achieve this, you need to subset the variables of interest for each type individually and submit them to a correlation test. You can use the square brackets for this **or** the [`subset` function](https://www.statmethods.net/management/subset.html), which is very intuitive for beginners. Please also visualize them with a scatterplot.

```{r}
#plot
ggplot(sandbox, aes(power, friendliness)) + 
  geom_point(aes(colour = pokemon))
```

```{r}
# subsetting using squared brackets
charizard <- sandbox[sandbox$pokemon == 'Charizard', ]
eevee <- sandbox[sandbox$pokemon == 'Eevee', ]
pikachu <- sandbox[sandbox$pokemon == 'Pikachu', ]

#cor.tests
charizard_r <- cor.test(charizard$friendliness, charizard$power)
charizard_r

eevee_r <- cor.test(eevee$friendliness, eevee$power)
eevee_r

pikachu_r <- cor.test(pikachu$friendliness, pikachu$power)
pikachu_r
```

```{r}
# subsetting in two steps using the subset function
charizard_subset <- sandbox["pokemon"]
charizard_subset <- subset(sandbox, pokemon == 'Charizard')

eevee_subset <- sandbox["pokemon"]
eevee_subset <- subset(sandbox, pokemon == 'Eevee')

pikachu_subset <- sandbox["pokemon"]
pikachu_subset <- subset(sandbox, pokemon == 'Pikachu')
```

# 4. Using what we have learned and applying it to the TheDataSet

Now we can unleash your new powers on this beast of a data set we have. 

Aggregate the reaction times and the success variable as a function of code(this is the participant variable).

```{r}
(aggData <- aggregate(cbind(rt, success) ~ code, FUN = mean, data = TheDataSet))
#success aggregation yields proportion of successful trials as TRUE is represented by 1 and FALSE by 0
```

Correlate the resulting reaction times and the success performance. This essentially checks if participants who were faster were also more accurate (positive) or less accurate (negative).

```{r}
#plot
ggplot(aggData, aes(rt, success)) +
  geom_point()

#QUESTION: positive and negative above should be switched?!?!
cor(aggData$rt, aggData$success)
cor.test(aggData$rt, aggData$success)
```


