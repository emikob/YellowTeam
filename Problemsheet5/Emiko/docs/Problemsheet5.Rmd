---
title: "Problemsheet 5"
author: "Emiko Bell"
date: "11/9/2020"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(psych)
library(tidyverse)
library(languageR)
library(here)
```

# 1. Contrast coding
Regression techniques and any other linear model (LM), such as multiple or logistic regression analysis, require factors of the experimental design to enter the model as contrasts. We discussed a simple 2-level case last week, but in order to gain a better theoretical and practical intuition about contrasts, we might need a few more examples. Contrasts (i.e., planned comparisons) between specific conditions or clusters of conditions are a very effective way to align our expectations with the statistical model of choice. We will keep using the lexdec data set from last session in order to get a better intuition about contrast coding in regression-based procedures. This will make our life a bit easier in the coming weeks.

Read-in the lexdec data set, subset the same variables as last time and save them into a data frame called data.

```{r}
data(lexdec)

selection <- c("Subject", "RT", "NativeLanguage", "Frequency","SubjFreq")
data <- lexdec[selection]
head(data)
```

## 1.1. Comparing two conditions/levels
Like last time, run a simple regression predicting reaction times, by the factor NativeLanguage.

```{r}
nlreg <- lm(RT ~ NativeLanguage, data = data)

summary(nlreg)
```

Now calculate the overall mean reaction time for each of the levels of the language, so English & Other. You will need these throughout this session, so make sure to get the mean values.

```{r}
mean(data$RT[data$NativeLanguage=="English"])
mean(data$RT[data$NativeLanguage=="Other"])
```

## 1.2. Treatment/Dummy contrasts
What do you see when you compare the means for each condition with the coefficients from the model? 1. The intercept is the mean for native speakers 2. The slope is the difference between the means of the two groups. Let this soak in for a moment - make sure it makes sense. As mentioned last session, the result is a consequence of the default contrast coding of the factor NativeLanguage. R assigns treatment/dummy contrasts to factors and orders their levels alphabetically. The first factor level (here: English) is coded as 0 and the second level (here: Other) is coded as 1.

```{r}
contrasts(data$NativeLanguage)
```

In regressions with categorical predictors, you can think of the unstandardized coefficients as difference scores - going one unit on x, relates to making a step from one level to the other. As there is no actual 0 point, the intercept values denotes the mean of the first level. Look back at the contrasts coding last session, and you will see that when you change the base of the treatment contrasts, the sign of the coefficients flips, but the value remains the same. Make sure you understand why this happens.

```{r}
contrasts(data$NativeLanguage)<-contr.treatment(2,base=2)
summary(lm(RT ~ NativeLanguage, data))$coefficients
```

## 1.3. Sum contrasts
There are many ways to define contrasts in order to test the critical comparisons in your design and we will get to know more and more as we go along, but for now we will take it slow. An important way to code contrasts are SUM CONTRASTS, in which one of the conditions is coded as −1 and the other as 1, You can imagine this as ‘centering’ the effects at the grand mean (i.e., the mean of the two group means). You can custom-make these kinds of contrasts in R, but for convenience and accuracy we will use specific functions.

```{r}
contr.sum(2)
```

In the same way as above, we can override the contrasts that our factor currently has with the ones we would like to use. Lets have a look at our current contrast coding:

```{r}
contrasts(data$NativeLanguage)
```

We can also always check how the levels of our factor are ordered:

```{r}
levels(data$NativeLanguage)
```

We of course already knew that R orders them alphabetically by default, but when doing a lot of data manipulations it is often useful to keep on top of these changes.

Now lets re-code the contrasts:

```{r}
(contrasts(data$NativeLanguage) <- contr.sum(2))
contrasts(data$NativeLanguage)
```

Now re-run the regression and evaluate how the coefficient and intercept have changed.

```{r}
newlm <- lm(RT ~ NativeLanguage, data = data)
summary(newlm)
```

The intercept is now the grand mean of the two group means.

```{r}
(mE <- mean(data$RT[data$NativeLanguage=="English"]))
(mO <- mean(data$RT[data$NativeLanguage=="Other"]))
mean(c(mE,mO))
```

For now we ignore the difference in the decimal places, which is due to unequal observations per group.

The slope codes the difference of the group associated with the first factor level and the GM. Note that the label of the slope coefficient is appended to the factor level (i.e., 1). Currently, the first level of NativeLanguage refers to English. Therefore, the predicted speed for the group of English subjects is 6.396 - 0.078 = 6.318 (cf. the output above). From this slope you can also infer that when you add 0.078 to the intercept (grand mean) you will get the predicted reaction time for Other.

To summarize, treatment contrasts and sum contrasts are two possible ways to compare the two groups, and they test different hypotheses. Treatment contrasts compare one or more means against a baseline condition, whereas sum contrasts allow us to determine whether a condition’s mean is significantly different from the GM (which in the two-group case implies also a significant difference to the second group). We will begin next session with a 3-level scenario, which makes things a bit more difficult.

# 2. Logistic regression
In comparison to multiple regression, logistic regression is used to model categorical variables - instead of continuous ones. In cases of only two categorical outcomes we speak of a binary logistic regression and whenever we use several predictor variables this is a multinomial | polychotomous logistic regression. One of the major conceptual milestones to check off here is that instead of predicting the value of a variable Y from predictor variables Xs - we instead predict the probability of Y occurring given known values of X. Logistic regression is in reality an ordinary regression using the logit as the response variable. The logit transformation allows for a linear relationship between the response variable and the coefficients.

## 2.1. New data set
To warm up to logistic regression, we will try out reading in a new data set again. Read in the concept_data txt file using read.table and save it to a data frame called GLMdata. Have a look at the summary of the data and get an idea of the different variables.

```{r}
GLMdata <- read.table(here("data/concept_data.txt"), sep = " ", header = TRUE, stringsAsFactors = TRUE)

head(GLMdata)
```

This data set offers one participant variable, one variable denoting which stimulus was used, two variables coding the experimental conditions and one DV. I guess you can already guess the nature of our DV, but nevertheless, please plot a very simple histogram.

*HAD TO RUN STRINGS AS FACTORS = TRUE*
*Can also run as.factor() or factor() to make it*

```{r}
ggplot(GLMdata, aes(x = dv)) + geom_histogram()
```

Lets get a sense of the data before we dive into the inferentials of logistic regression. Get the mean dv as a function of par2. Start using aggregate for operations like this.

```{r}
aggregate(dv ~ par2, GLMdata, FUN = mean)
```

Next, we want to check how the contrasts are currently coded for par2.

```{r}
contrasts(GLMdata$par2)
```
*(There's an error saying contrasts only apply to factors??) I had to specify it as factors*

## 2.1. Performing a logistic regression
Now we are ready to conduct a logistic regression! Lets dive right into it and start making sense of the model output.

```{r}
m1 <- glm(dv~par2,
          family = binomial(), GLMdata)
summary(m1)
```

### 2.1.1. The z-statistic
In liner regression, we used the estimated regression coefficients and their standard errors to compute a t-statistic. Here we are using the analogous z-statistic, which follows the normal distribution. So you can use this to evaluate if the coefficient of the predictor is significantly different from zero. We should remember here, that the z-statistic is generally being underestimated when the regression coefficient is large, because the standard errors tend to become inflated. In this test we are more likely to make a Type II error.

### 2.1.2. Odds ratio
Another conceptual milestone and something which is very important for the interpretation of logistic regression is the value of the odds ratio. This indicates the change in odds resulting from a unit change in the predictor and is denoted by the exponential of the coefficient (==exp(Estimate)).

When predicting a categorial outcome (smth happening (1) vs. nothing happening (0)), the odds of an event occurring are defined as the probability of an event occurring divided by the probability of that event not occurring. In order to calculate the change in odds that result from a unit change in the predictor, we must first calculate the odds of one of the events (e.g. A), the odds of the other event (e.g. B) and finally calculate the proportionate change in these two odds. This change is what we call odds ratio. When the value is greater than 1, then we know that as the predictor increases, the odds of the outcome increase. A value less than 1 indicates that as the predictor increases, the odds of the outcome occurring decrease.

Wow, this is a lot! Somewhat intuitive maybe, but not for me. I need to touch the data before this makes sense to me. So lets try finding this theoretical construct in our model output. Currently, the coefficient values are represented in logits - the logarithm of the odds ratio of dv occurring. Thus, the exponential of the coefficients will yield the odd ratios. Lets evaluate this while at the same time reminding us of the way we have coded the contrasts and the model output.

```{r}
contrasts(GLMdata$par2)
summary(m1)
exp(m1$coefficients)
```

So here we can say that the odds of 1 occurring given B are approx. 6-times higher than those of 1 occurring given A.

So, if for example, the dependent variable here was accuracy (1 = Correct vs. 0 = Incorrect) and the predictor (par2) was Alcohol intake (A = Drunk vs. B = Sober), we could say that the probability of you being correct when you are sober is 6-times higher compared to when you are drunk. Please take the time to compare all the values in the outputs above to make sure that this makes sense.

When we increase x by one unit the odds are multiplied by exp(coefficient). Odds increase when the probability of success increases, so that if your coefficient is positive, increasing x will increase your probability. Lets try to reconstruct the intercept and coefficient:

```{r}
aggregate(dv~par2,GLMdata,mean) # probability of 1 given A and 1 given B
0.284/0.716 # computing the odds for 1 given A (== intercept)
0.716/0.284 # computing the odds for 1 given B
2.521127/0.396648 # odds ratio for 1 as you go from A to B (== slope)
```

### 2.1.3. Deviance
The deviance statistic gives us the overall fit of the model - larger values indicate poorer-fitting. The Null deviance indicates the deviance of the baseline model, so the one which uses category frequency as its model. The residual deviance is the deviance for the model we fitted to the data. So this value should be lower then then Null deviance, indicating a better fit. Conceptually, this means that adding par2 to the model, made the model better in predicting dv.

We can test if we get a better fit compared to the baseline model, in the same way in which we test different models against each other.

```{r}
anova(m1, test="Chisq")
```

### 2.1.4. R²
As you might have noticed, R did not provide us with an R² value. We will stumble upon curiosities like this from time to time in this lecture. There actually is a good reason not to report R². There is various packages and functions which will provide you with the optimal tools to derive an R² value - I will leave it up to you to find one in case it is required from you. I will provide you with the means of calculating Hosmer & Lemeshow’s R² in order to satisfy APA guidelines:

```{r}
(r_squared <- (m1$null.deviance-m1$deviance)/m1$null.deviance)
```

2.1.5. Confidence interval
Finally, as with linear regression, we can easily compute the confidence intervals for our coefficients.

```{r}
confint(m1) 
```


or wrap the exp() function around it for the odds ratio values
The critical thing to look out for here is that the interval does not cross 1. Values greater than 1 mean that as the predictor variable increases, so do the odds of dv.

### 2.1.6. Predicting probabilities
This is a bit trivial in our simple case, but we can easily calculate point predictions for our dependent variable for each value of a predictor.

```{r}
newdf <- data.frame(par2="B")
predict(m1, newdf, type="response")
```

## 2.2. Adding a second predictor
Now, its your turn to get active. Add par1 as an additive predictor to the model. Is par1 a significant predictor? How about the overall model? Is it significantly better in explaining the data compared to the previous model? Use the anova function to compare the two models. All tools for this calculation are available to you from this and the last session.

```{r}
m2 <- glm(dv~par2 + par1,
          family = binomial(), GLMdata)
summary(m2)
exp(m2$coefficients)

anova(m1, m2, test = "Chisq")
```

Par1 is not a significant predictor, and the second model is not a better fit.

Play around with different values for par1 and par2, in order to gain an intuition of the predicted probabilities of logistic regressions.

```{r}
newdf <- data.frame(par1=3, par2="B")
predict(m2, newdf, type="response")
aggregate(dv~par2+par1,GLMdata,mean)
```


# 3. Teaser: aggregating and plotting
Visualization of data is the number one tool for extracting meaning from your results. We were having a lot of fun running various logistic regressions, but sometimes a quick look at the data will reveal a relationship that we might have completely missed. Think about what we have been modeling and what we have not been modeling. We will talk about more complex relationships in upcoming sessions.

```{r}
agg <- aggregate(dv~par2+par1,GLMdata,mean)
ggplot(agg, aes(x=par1, y=dv, color=par2))+geom_line()
```

